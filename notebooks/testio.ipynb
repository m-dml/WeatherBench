{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from time import time\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "MB = 1048576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = '/gpfs/work/greenber/5_625deg_all_zscored.npy'  # original data to be loaded with xarray\n",
    "outputdir = '/gpfs/work/greenber/iotest/'\n",
    "assert os.path.exists(inputfile)\n",
    "assert(os.path.isdir(outputdir))\n",
    "\n",
    "n_files = 16\n",
    "\n",
    "target_read_size = 256 * MB  # bytes\n",
    "\n",
    "rewrite = False  # replace existing files\n",
    "\n",
    "tmax = 60  # test time in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350640 data points, 192512 values per data point, 4 bytes per value\n",
      "Dividing data into 16 files of up to 21915 data points each\n"
     ]
    }
   ],
   "source": [
    "indata = np.load(inputfile, mmap_mode='r')\n",
    "dtype = indata.dtype\n",
    "n_points = indata.shape[0]\n",
    "datapointsize = np.prod(indata.shape[1:])\n",
    "datapointbytes = datapointsize * indata.dtype.itemsize\n",
    "\n",
    "points_per_file = int(np.ceil(n_points / n_files))\n",
    "\n",
    "testfiles = [os.path.join(outputdir, f'{i + 1}_of_{n_files}.dat') for i in range(n_files)]\n",
    "\n",
    "points_per_read = np.maximum(1, int(np.round(target_read_size / datapointbytes)))\n",
    "\n",
    "print(f'{n_points} data points, {datapointsize} values per data point, {indata.dtype.itemsize} bytes per value')\n",
    "print(f'Dividing data into {n_files} files of up to {points_per_file} data points each')\n",
    "del indata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Divide data into multiple files on disk, grouping together everything for the same time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready.\n"
     ]
    }
   ],
   "source": [
    "for i, outputfile in enumerate(testfiles):  # use multiprocessing here too?\n",
    "    \n",
    "    i_start, i_end = i * points_per_file, np.minimum((i + 1) * points_per_file, n_points)\n",
    "    \n",
    "    # skip existing files\n",
    "    if os.path.exists(outputfile) and not rewrite:\n",
    "        expected_bytes = (i_end - i_start) * datapointsize * dtype.itemsize\n",
    "        actual_bytes = os.path.getsize(outputfile)\n",
    "        if expected_bytes == actual_bytes:\n",
    "            continue\n",
    "    \n",
    "    print(f'Creating file {outputfile}')\n",
    "    indata = np.load(inputfile, mmap_mode='r')\n",
    "    y = np.empty(dtype=indata.dtype, shape=(i_end - istart, *indata.shape[1:]))  # read data from disk\n",
    "    y[:] = indata[i_start:i_end]\n",
    "    y_out = np.memmap(outputfile, mode='w+', dtype=y.dtype, shape=y.shape)\n",
    "    y_out[:] = y\n",
    "    y_out.flush()\n",
    "    # clear memory\n",
    "    del indata, y, y_out\n",
    "    print('done with file')\n",
    "print('Data is ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define infinite random permutation through each file's data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infinite_randperm(n):\n",
    "    while True:\n",
    "        ii = np.random.permutation(n)\n",
    "        for idx in ii:\n",
    "            yield idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define task of reading each file endlessly in random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readloop(file, datapointsize=None, dtype=np.float32, tmax=60, rngseed=None, read_sorted=True,\n",
    "             mmap=True):\n",
    "    assert file is not None and datapointsize is not None\n",
    "    assert os.path.exists(file)\n",
    "    \n",
    "    n = os.path.getsize(file) / (datapointsize * dtype.itemsize)\n",
    "    assert np.abs(n - np.round(n)) / n < 1e-6 and n > 0\n",
    "    n = int(np.round(n))\n",
    "    \n",
    "    if rngseed is None:\n",
    "        np.random.seed(int.from_bytes(os.urandom(4), byteorder='little'))\n",
    "    else:\n",
    "        np.random.seed(rngseed)\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    x = np.empty((points_per_read, datapointsize), dtype=dtype)\n",
    "    if mmap:\n",
    "        y = np.memmap(file, mode='r', dtype=dtype, shape=(n, datapointsize))\n",
    "    else:\n",
    "        y = open(file, 'rb')\n",
    "    \n",
    "    t_read, bytes_read = [], []\n",
    "    \n",
    "    read_order = infinite_randperm(n)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        t1 = time()\n",
    "        \n",
    "        ii = [next(read_order) for _ in range(points_per_read)]\n",
    "        if read_sorted:\n",
    "            ii = np.sort(ii)\n",
    "        \n",
    "        if mmap:\n",
    "            x[:len(ii)] = y[ii]\n",
    "        else:\n",
    "            for u, offset in enumerate(ii * datapointsize * dtype.itemsize):\n",
    "                y.seek(offset, 0)\n",
    "                x[u] = np.frombuffer(y.read(datapointsize * dtype.itemsize), dtype=dtype)\n",
    "        \n",
    "        t2 = time()\n",
    "        \n",
    "        t_read.append(t2 - t1)\n",
    "        bytes_read.append(len(ii) * datapointsize * dtype.itemsize)\n",
    "        \n",
    "        if t2 - t0 > tmax:\n",
    "            break\n",
    "    \n",
    "    if not mmap:\n",
    "        y.close()\n",
    "    return np.array(t_read), np.array(bytes_read)\n",
    "        \n",
    "f = partial(readloop, datapointsize=datapointsize, dtype=dtype, tmax=60)\n",
    "fr = partial(readloop, datapointsize=datapointsize, dtype=dtype, tmax=60, mmap=False)\n",
    "fu = partial(readloop, datapointsize=datapointsize, dtype=dtype, tmax=60, read_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1324.7933786262479 MB / s with 1 process\n"
     ]
    }
   ],
   "source": [
    "tr, br = f(testfiles[0])\n",
    "\n",
    "print(f'{(np.sum(br)/np.sum(tr)) / MB} MB / s with 1 process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1331.9092235947592 MB / s with 1 process, no memmap\n"
     ]
    }
   ],
   "source": [
    "tr_r, br_r = fr(testfiles[0])\n",
    "\n",
    "print(f'{(np.sum(br)/np.sum(tr)) / MB} MB / s with 1 process, no memmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(processes=n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pool.map(f, testfiles)\n",
    "print(f'{np.sum([np.sum(br)/np.sum(tr) for tr, br in results]) / MB} MB / s with {len(results)} processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pool.map(fr, testfiles)\n",
    "print(f'{np.sum([np.sum(br)/np.sum(tr) for tr, br in results]) / MB} MB / s with {len(results)} processes, no memmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"tr_indiv, br_indiv = [], []\n",
    "for testfile in testfiles:\n",
    "    tr, br = f(testfiles)\n",
    "    tr_indiv.append(tr)\n",
    "    br_indiv.append(br)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wbtorch",
   "language": "python",
   "name": "wbtorch"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
