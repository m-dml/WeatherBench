{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from src.pytorch.util import named_network, init_torch_device\n",
    "from src.pytorch.Dataset import Dataset\n",
    "from src.pytorch.train import train_model\n",
    "from torch.utils.data import RandomSampler \n",
    "\n",
    "#directories\n",
    "datadir = '/gpfs/work/nonnenma/data/forecast_predictability/weatherbench/5_625deg/'\n",
    "results_dir = '/gpfs/work/nonnenma/results/forecast_predictability/weatherbench/5_625deg/'\n",
    "assert os.path.exists(datadir) and os.path.exists(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = {'geopotential': ('z', [500])}  # input variables/levels\n",
    "if False:\n",
    "    var_dict =  {'geopotential': ('z', [100, 200, 500, 850, 1000]), \n",
    "                 'temperature': ('t', [100, 200, 500, 850, 1000]), \n",
    "                 'u_component_of_wind': ('u', [100, 200, 500, 850, 1000]), \n",
    "                 'v_component_of_wind': ('v', [100, 200, 500, 850, 1000]), \n",
    "                 'constants': ['lsm','orography','lat2d']}\n",
    "\n",
    "target_vars = ['geopotential']  # output variables\n",
    "target_levels = [500]  # output levels\n",
    "\n",
    "lead_time = 3 * 24  # hours in the future for predictions\n",
    "\n",
    "train_years = ('1979', '2015')\n",
    "valid_years = ('2016', '2016')\n",
    "test_years = ('2017', '2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network and optimization strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'simpleResnet' # 'simpleResnet', 'tvfcnResnet50', 'cnnbn', 'Unetbn'\n",
    "batch_size = 32\n",
    "max_epochs = 5\n",
    "max_patience = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_speed = False\n",
    "train_again = False  # if False and saved trained network exists from a previous run, load instead of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which device to use\n",
    "device = init_torch_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.merge([xr.open_mfdataset(f'{datadir}/{var}/*.nc', combine='by_coords')\n",
    "               for var in var_dict.keys()],\n",
    "              fill_value=0)  # For the 'tisr' NaNs\n",
    "ds = ds.chunk({'time' : np.sum(ds.chunks['time']), 'lat' : ds.chunks['lat'], 'lon': ds.chunks['lon']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify year ranges\n",
    "ds_train, ds_valid, ds_test = ds.sel(time=slice(*train_years)), ds.sel(time=slice(*valid_years)), ds.sel(time=slice(*test_years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap in Dataset object for batching etc.\n",
    "args_D = [var_dict, lead_time]\n",
    "kwargs_D = dict(target_vars=target_vars, target_levels=target_levels, normalize=True)\n",
    "\n",
    "D_train = Dataset(ds_train, *args_D, norm_subsample=30000, randomize_order=True, **kwargs_D)\n",
    "kwargs_D.update(dict(mean=D_train.mean, std=D_train.std))\n",
    "D_valid = Dataset(ds_valid, *args_D, randomize_order=False, **kwargs_D)\n",
    "D_test = Dataset(ds_valid, *args_D,  randomize_order=False, **kwargs_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(D_train, batch_size=batch_size, drop_last=True)\n",
    "validation_loader = torch.utils.data.DataLoader(D_valid, batch_size=batch_size, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels = len(D_train.data.level.level)\n",
    "print(f'{n_channels} total input channels')\n",
    "base_filename = f'{n_channels}D_fc{model_name}_{lead_time//24}d_pytorch' # file name for saving/loading prediction model\n",
    "model_filename = base_filename + '.pt' # file name for saving/loading prediction model\n",
    "training_outputs_filename = base_filename + '_training.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, model_forward = named_network(model_name, n_channels, len(target_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_batch_speed:\n",
    "    from time import time\n",
    "    t0 = time()\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "    print(f'{time() - t0} seconds per epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_again or not os.path.exists(training_outputs_filename):\n",
    "                                     \n",
    "    training_outputs = train_model(model, train_loader, validation_loader, device, model_forward,\n",
    "                                   max_epochs=max_epochs, max_patience=max_patience)\n",
    "    torch.save(model.state_dict(), results_dir + model_filename)\n",
    "    np.save(training_outputs, training_outputs_filename)\n",
    "                                     \n",
    "else:  # load model from disk\n",
    "\n",
    "    model.load_state_dict(torch.load(state_dict_file, map_location=torch.device(device)))\n",
    "    training_outputs = np.load(training_outputs_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
