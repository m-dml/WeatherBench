{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define setup\n",
    "- single field (!): geopotential\n",
    "- all levels, i.e. 11 z-levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.pytorch.util import init_torch_device\n",
    "\n",
    "device = init_torch_device()\n",
    "datadir = '/gpfs/work/nonnenma/data/forecast_predictability/weatherbench/5_625deg/'\n",
    "res_dir = '/gpfs/work/nonnenma/results/forecast_predictability/weatherbench/5_625deg/'\n",
    "\n",
    "batch_size = 32\n",
    "lead_time = 72\n",
    "train_years = ('2015', '2015')\n",
    "\n",
    "var_dict = {'geopotential': ('z', [   1,   10,  100,  200,  300,  400,  500,  600,  700,  850, 1000])}\n",
    "target_var_dict = {'geopotential': 500} # atm this is only for the Dataset to initialize\n",
    "\n",
    "past_times = [] # no extra time-shifted inputs for now\n",
    "verbose = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunks ((8760,), (11,), (32,), (64,))\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from src.pytorch.Dataset import Dataset_dask, Dataset_xr\n",
    "\n",
    "x = xr.merge(\n",
    "[xr.open_mfdataset(f'{datadir}/{var}/*.nc', combine='by_coords')\n",
    " for var in var_dict.keys()],\n",
    "fill_value=0  # For the 'tisr' NaNs\n",
    ")\n",
    "dg_train = Dataset_dask(x.sel(time=slice(train_years[0], train_years[1])), var_dict, lead_time, \n",
    "                   normalize=False, res_dir=res_dir, train_years=train_years,\n",
    "                   target_var_dict=target_var_dict, past_times=past_times, verbose=verbose)\n",
    "\n",
    "print('chunks', dg_train.data.chunks)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    X_stack, Y_stack = dask.compute(dask.array.stack([X for X,_ in batch]), \n",
    "                                    dask.array.stack([y for _,y in batch]))\n",
    "    X_stack = torch.as_tensor(X_stack, device='cpu')\n",
    "    Y_stack = torch.as_tensor(Y_stack, device='cpu')\n",
    "\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    if torch.utils.data.get_worker_info() is not None:\n",
    "        # If we're in a background process, concatenate directly into a\n",
    "        # shared memory tensor to avoid an extra copy\n",
    "        storage = X_stack.storage()._new_shared(X_stack.numel())\n",
    "        out = X_stack.new(storage)\n",
    "        return torch.stack(batch, 0, out=out)\n",
    "    \"\"\"\n",
    "\n",
    "    return (X_stack, Y_stack)\n",
    "\n",
    "num_workers = int(train_years[1]) - int(train_years[0]) + 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg_train,\n",
    "    #pin_memory=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100\n",
    "print_every = 10\n",
    "\n",
    "import time\n",
    "\n",
    "def do_dummy_epoch(train_loader):\n",
    "    # check I/O speed on single (empty) epoch\n",
    "    num_steps = 1\n",
    "    t = time.time()\n",
    "    for batch in train_loader:\n",
    "        if np.mod(num_steps, print_every) == 0 or num_steps == 1:\n",
    "            print(f\"- batch #{num_steps}, time: {'{0:.2f}'.format(time.time() - t)}\")\n",
    "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "        out = (inputs.shape, targets.shape)\n",
    "        #print(out)\n",
    "        num_steps +=1\n",
    "        if num_steps > max_steps:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xr-generated Dask arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch #1, time: 1.78\n",
      "- batch #10, time: 10.47\n",
      "- batch #20, time: 20.04\n",
      "- batch #30, time: 29.62\n",
      "- batch #40, time: 39.18\n",
      "- batch #50, time: 48.61\n",
      "- batch #60, time: 58.39\n",
      "- batch #70, time: 67.94\n",
      "- batch #80, time: 77.40\n",
      "- batch #90, time: 86.97\n",
      "- batch #100, time: 96.52\n"
     ]
    }
   ],
   "source": [
    "do_dummy_epoch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create numpy array for direct comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 2015\n"
     ]
    }
   ],
   "source": [
    "var = list(var_dict.keys())[0]\n",
    "for year in range(int(train_years[0]), int(train_years[1])+1):\n",
    "    print('year', year)\n",
    "    x = xr.open_mfdataset(f'{datadir}/{var}/*{year}*.nc', combine='by_coords')\n",
    "    darray = x[var_dict[var][0]].values\n",
    "    np.save(res_dir + var + f'/geopotential_{year}_5_625deg', darray) # all levels for single year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dask array directly from numpy array \n",
    "- Remember: dask itself does lazy evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1: load full numpy array into memory, create dask array from that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch #1, time: 0.54\n",
      "- batch #10, time: 4.34\n",
      "- batch #20, time: 8.49\n",
      "- batch #30, time: 12.71\n",
      "- batch #40, time: 16.87\n",
      "- batch #50, time: 21.03\n",
      "- batch #60, time: 25.21\n",
      "- batch #70, time: 29.36\n",
      "- batch #80, time: 33.56\n",
      "- batch #90, time: 37.76\n",
      "- batch #100, time: 42.01\n"
     ]
    }
   ],
   "source": [
    "dg_train.data.data = dask.array.from_array(\n",
    "    np.load(res_dir + 'geopotential/' + 'geopotential_2015_5_625deg.npy', allow_pickle=False),\n",
    "    chunks=dg_train.data.chunks)\n",
    "\n",
    "do_dummy_epoch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2: store parts for dask array from disk, then create dask array only from pointer to disk (no pre-loading!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "darray = np.load(res_dir + 'geopotential/' + 'geopotential_2015_5_625deg.npy', allow_pickle=False)\n",
    "darray = dask.array.from_array(darray, chunks=dg_train.data.chunks)\n",
    "dask.array.to_npy_stack(res_dir + 'geopotential/', darray) # creates a new file '0.npy' along with an 'info'\n",
    "del darray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch #1, time: 2.77\n",
      "- batch #10, time: 8.21\n",
      "- batch #20, time: 14.02\n",
      "- batch #30, time: 19.87\n",
      "- batch #40, time: 25.65\n",
      "- batch #50, time: 31.50\n",
      "- batch #60, time: 37.42\n",
      "- batch #70, time: 43.42\n",
      "- batch #80, time: 49.27\n",
      "- batch #90, time: 55.08\n",
      "- batch #100, time: 60.99\n"
     ]
    }
   ],
   "source": [
    "dg_train.data.data = dask.array.from_npy_stack(res_dir + '/geopotential')\n",
    "\n",
    "do_dummy_epoch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy approaches (ditching xarray, Dask)\n",
    "### Remember: will mostly load all into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### directly read from numpy array (but dask.array in collate_fn..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object): # dummy object to house fields expected by Dataset.__iter__()\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.chunks = ((8760,), (11,), (32,), (64,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch #1, time: 0.76\n",
      "- batch #10, time: 1.05\n",
      "- batch #20, time: 1.45\n",
      "- batch #30, time: 1.76\n",
      "- batch #40, time: 2.07\n",
      "- batch #50, time: 2.38\n",
      "- batch #60, time: 2.76\n",
      "- batch #70, time: 3.14\n",
      "- batch #80, time: 3.54\n",
      "- batch #90, time: 3.92\n",
      "- batch #100, time: 4.30\n"
     ]
    }
   ],
   "source": [
    "dg_train.data = Data()\n",
    "dg_train.data.data = np.load(res_dir + 'geopotential/' + 'geopotential_2015_5_625deg.npy', allow_pickle=False)\n",
    "\n",
    "do_dummy_epoch(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pure numpy (rewrite collate_fn & pass to train_loader). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_np(batch):\n",
    "    X_stack, Y_stack = (np.stack([X for X,_ in batch]), np.stack([y for _,y in batch]))\n",
    "    X_stack = torch.as_tensor(X_stack, device='cpu')\n",
    "    Y_stack = torch.as_tensor(Y_stack, device='cpu')\n",
    "    return (X_stack, Y_stack)\n",
    "\n",
    "train_loader_np = torch.utils.data.DataLoader(\n",
    "    dg_train,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn_np,\n",
    "    drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- batch #1, time: 0.68\n",
      "- batch #10, time: 0.73\n",
      "- batch #20, time: 0.78\n",
      "- batch #30, time: 0.83\n",
      "- batch #40, time: 0.88\n",
      "- batch #50, time: 0.93\n",
      "- batch #60, time: 0.98\n",
      "- batch #70, time: 1.03\n",
      "- batch #80, time: 1.08\n",
      "- batch #90, time: 1.13\n",
      "- batch #100, time: 1.18\n"
     ]
    }
   ],
   "source": [
    "dg_train.data = Data()\n",
    "dg_train.data.data = np.load(res_dir + 'geopotential/' + 'geopotential_2015_5_625deg.npy', allow_pickle=False)\n",
    "\n",
    "do_dummy_epoch(train_loader_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy memmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8760, 11, 32, 64)\n",
      "- batch #1, time: 2.51\n",
      "- batch #10, time: 2.56\n",
      "- batch #20, time: 2.61\n",
      "- batch #30, time: 2.66\n",
      "- batch #40, time: 2.71\n",
      "- batch #50, time: 2.76\n",
      "- batch #60, time: 2.81\n",
      "- batch #70, time: 2.86\n",
      "- batch #80, time: 2.91\n",
      "- batch #90, time: 2.96\n",
      "- batch #100, time: 3.01\n"
     ]
    }
   ],
   "source": [
    "dg_train.data = Data()\n",
    "\n",
    "darray = np.memmap(res_dir + 'geopotential/' + 'geopotential_2015_5_625deg.npy', \n",
    "                   dtype=np.float32, mode='r', shape=tuple(np.concatenate(dg_train.data.chunks)))\n",
    "dg_train.data.data = darray\n",
    "print(darray.shape)\n",
    "\n",
    "do_dummy_epoch(train_loader_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
