{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# dev\n",
    "- reimplementing the 'Quickstart' notebook from S. Rasp for training in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from src.train_nn_pytorch import Dataset\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('using CUDA !')\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "\n",
    "lead_time = 3*24\n",
    "var_dict = {'z': None, 't': None}\n",
    "batch_size = 32\n",
    "\n",
    "datadir = '/gpfs/work/nonnenma/data/forecast_predictability/weatherbench/5_625deg/'\n",
    "res_dir = '/gpfs/work/nonnenma/results/forecast_predictability/weatherbench/5_625deg/'\n",
    "\n",
    "z500 = xr.open_mfdataset(f'{datadir}geopotential_500/*.nc', combine='by_coords')\n",
    "t850 = xr.open_mfdataset(f'{datadir}temperature_850/*.nc', combine='by_coords')\n",
    "dataset_list = [z500, t850]\n",
    "x = xr.merge(dataset_list, compat='override')\n",
    "x = x.chunk({'time' : np.sum(x.chunks['time']), 'lat' : x.chunks['lat'], 'lon': x.chunks['lon']})\n",
    "n_channels = len(dataset_list) # = 1 if only loading one of geopotential Z500 and temperature T850\n",
    "\n",
    "# tbd: separating train and test datasets / loaders should be avoidable with the start/end arguments of Dataset!\n",
    "\n",
    "dg_train = Dataset(x.sel(time=slice('2010', '2015')), var_dict, lead_time, normalize=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg_train,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True)\n",
    "\n",
    "dg_validation =  Dataset(x.sel(time=slice('2016', '2016')), var_dict, lead_time,\n",
    "                        mean=dg_train.mean, std=dg_train.std, normalize=True)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dg_validation,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "    print((inputs.shape, targets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_nn_pytorch import SimpleCNN\n",
    "\n",
    "net = SimpleCNN(filters=[64, 64, 64, 64, n_channels], kernels=[5, 5, 5, 5, 5], \n",
    "          channels=n_channels, activation=torch.nn.functional.elu, mode='standard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    n_epochs, max_patience = 200, 20\n",
    "    best_loss, patience = np.inf, max_patience\n",
    "    best_state_dict = {}\n",
    "\n",
    "    epoch = 0\n",
    "    while True:\n",
    "\n",
    "        epoch += 1\n",
    "        if epoch > n_epochs:\n",
    "            break\n",
    "\n",
    "        print(f'epoch #{epoch}')\n",
    "        # Train for a single epoch.\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "            loss = F.mse_loss(net.forward(inputs), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Track convergence on validation set.\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            nb = 0\n",
    "            for batch in validation_loader:\n",
    "                inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "                val_loss += F.mse_loss(net.forward(inputs), targets)\n",
    "                nb += 1\n",
    "        val_loss /= nb\n",
    "        print(f'epoch #{epoch} || loss (last batch) {loss} || validation loss {val_loss}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            patience = max_patience\n",
    "            best_loss = val_loss\n",
    "            best_state_dict = deepcopy(net.state_dict())\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience == 0:\n",
    "            net.load_state_dict(best_state_dict)\n",
    "            break\n",
    "\n",
    "    torch.save(net.state_dict(), res_dir + 'test_fccnn_3d_pytorch.pt')\n",
    "\n",
    "else:\n",
    "    net.load_state_dict(torch.load(res_dir + 'test_fccnn_3d_pytorch.pt', map_location=torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create a prediction and compute score\n",
    "\n",
    "Now that we have a model (albeit a crappy one) we can create a prediction. For this we need to create a forecast for each forecast initialization time in the testing range (2017-2018) and unnormalize it. We then convert the forecasts to a Xarray dataset which allows us to easily compute the RMSE. All of this is taken care of in the `create_predictions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_nn_pytorch import create_predictions\n",
    "\n",
    "dg_test =  Dataset(x.sel(time=slice('2017', '2018')), var_dict, lead_time,\n",
    "                        mean=dg_train.mean, std=dg_train.std, normalize=True)\n",
    "preds = create_predictions(net, dg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.score import compute_weighted_rmse, load_test_data\n",
    "\n",
    "z500_test = load_test_data(f'{datadir}geopotential_500/', 'z')\n",
    "t850_test = load_test_data(f'{datadir}temperature_850/', 't')\n",
    "rmse_z = compute_weighted_rmse(preds.z, z500_test.isel(time=slice(lead_time, None))).load()\n",
    "rmse_t = compute_weighted_rmse(preds.t, t850_test.isel(time=slice(lead_time, None))).load()\n",
    "rmse_z, rmse_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "var_names = {'z' : 'geopotential at 500hPa',\n",
    "             't' : 'temperature at 850hPa'}\n",
    "idx = [0, 100, 500]\n",
    "for i in idx:\n",
    "    pred = net.forward(torch.tensor(dg_test[[i]][0],requires_grad=False).to(device)).detach().numpy()\n",
    "    #pred = pred*dg_train.std.values[None,:,None,None] + dg_train.mean.values[None,:,None,None]\n",
    "    plt.figure(figsize=(16,6))\n",
    "    for j in range(2):\n",
    "        plt.subplot(1,2,j+1)\n",
    "        plt.imshow(np.vstack((dg_test[[i]][1][0,j,:,:], pred[0,j,:,:], dg_test[[i]][0][0,j,:,:])))\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (1*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (2*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.yticks([pred.shape[2]//2, 3*pred.shape[2]//2, 5*pred.shape[2]//2], \n",
    "                   [f'+{lead_time}h true', f'+{lead_time}h est.', 'state'])\n",
    "        plt.axis([-0.5, pred.shape[3]-0.5, -0.5, 3*pred.shape[2]-0.5])\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(var_names[list(dg_train.var_dict.keys())[j]])\n",
    "        plt.title(dg_train.data.time.isel(time=i).values)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The End\n",
    "\n",
    "This is the end of the quickstart guide. Please refer to the Jupyter notebooks in the `notebooks` directory for more examples. If you have questions, feel free to ask them as a Github Issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
