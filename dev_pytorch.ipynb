{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# dev\n",
    "- reimplementing the 'Quickstart' notebook from S. Rasp for training in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from src.train_nn_pytorch import Dataset\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('using CUDA !')\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "\n",
    "datadir = '/gpfs/work/nonnenma/data/forecast_predictability/weatherbench/5_625deg/'\n",
    "res_dir = '/gpfs/work/nonnenma/results/forecast_predictability/weatherbench/5_625deg/'\n",
    "\n",
    "model_fn = '2D_fcUnet_3d_pytorch.pt' # file name for saving/loading prediction model\n",
    "\n",
    "lead_time = 3*24\n",
    "batch_size = 32\n",
    "\n",
    "\"\"\"\n",
    "# geopotential and tempearture each at 11 levels \n",
    "# regression target is Z500 (level index i=6) and T850 (level index 9, stacked vector index 9+11=20)\n",
    "geop = xr.open_mfdataset(f'{datadir}geopotential/*.nc', combine='by_coords')\n",
    "temp = xr.open_mfdataset(f'{datadir}temperature/*.nc', combine='by_coords')\n",
    "\n",
    "# specific humidity, and wind component (u,v) volumes, each at 11 levels \n",
    "sphq = xr.open_mfdataset(f'{datadir}specific_humidity/*.nc', combine='by_coords')\n",
    "cowu = xr.open_mfdataset(f'{datadir}u_component_of_wind/*.nc', combine='by_coords')\n",
    "cowv = xr.open_mfdataset(f'{datadir}v_component_of_wind/*.nc', combine='by_coords')\n",
    "\"\"\"\n",
    "\n",
    "# geopotential and tempearture each at target pressure levels \n",
    "z500 = xr.open_mfdataset(f'{datadir}geopotential_500/*.nc', combine='by_coords')\n",
    "t850 = xr.open_mfdataset(f'{datadir}temperature_850/*.nc', combine='by_coords')\n",
    "\n",
    "# incident solar radiation and cloud cover fields, each single-level\n",
    "tisr = xr.open_mfdataset(f'{datadir}toa_incident_solar_radiation/*.nc', combine='by_coords')\n",
    "clou = xr.open_mfdataset(f'{datadir}total_cloud_cover/*.nc', combine='by_coords')\n",
    "\n",
    "# constants: orography, land-sea mask, soil type, lat2d and lon2d (each single-level)\n",
    "cnst = xr.open_mfdataset(f'{datadir}constants/*.nc', combine='by_coords')\n",
    "template = tisr.tisr\n",
    "T = len(template.time.values)\n",
    "dataarrays = {}\n",
    "for var in [cnst.orography, cnst.lsm, cnst.slt, cnst.lat2d, cnst.lon2d]:\n",
    "    # manipulating stride would be preferable over np.stride, but unsure if xarray accepts that\n",
    "    values = np.tile(var.values.reshape(1,*var.values.shape), (T, 1, 1)).astype(np.float32)\n",
    "    dataarrays[var.name] = xr.DataArray(values, coords=template.coords, dims=template.dims, \n",
    "                                name=var.name,indexes=template.indexes)\n",
    "cnst = xr.Dataset(data_vars=dataarrays)\n",
    "\n",
    "\"\"\"\n",
    "# merging different fields into single dataset (this can take long, and a lot of RAM!)\n",
    "x = xr.merge([z500, t850, tisr, clou, cnst], compat='override', fill_value=0) # fill_value for tisr !\n",
    "x = x.chunk({'time' : np.sum(x.chunks['time']), \n",
    "             'lat' : x.chunks['lat'], 'lon': x.chunks['lon']})\n",
    "\n",
    "# dictionary of used variables and their levels for Dataset() objects\n",
    "var_dict = {'z': None,          # target\n",
    "            't': None,          # target\n",
    "            'tisr' : None,      # extra field\n",
    "            'tcc' : None,       # extra field\n",
    "            'orography' : None, # constant\n",
    "            'lsm' : None,       # constant\n",
    "            'slt' : None,       # constant\n",
    "            'lat2d' : None,     # constant\n",
    "            'lon2d': None}      # constant\n",
    "\"\"\"\n",
    "\n",
    "# merging different fields into single dataset (this can take long, and a lot of RAM!)\n",
    "x = xr.merge([z500, t850], compat='override', fill_value=0) # fill_value for tisr !\n",
    "x = x.chunk({'time' : np.sum(x.chunks['time']), \n",
    "             'lat' : x.chunks['lat'], 'lon': x.chunks['lon']})\n",
    "\n",
    "# dictionary of used variables and their levels for Dataset() objects\n",
    "var_dict = {'z': None,          # target\n",
    "            't': None}          # target\n",
    "\n",
    "# tbd: separating train and test datasets / loaders should be avoidable with the start/end arguments of Dataset!\n",
    "dg_train = Dataset(x.sel(time=slice('1979', '2015')), var_dict, lead_time, normalize=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg_train,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True)\n",
    "\n",
    "dg_validation =  Dataset(x.sel(time=slice('2016', '2016')), var_dict, lead_time,\n",
    "                        mean=dg_train.mean, std=dg_train.std, normalize=True)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dg_validation,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False)\n",
    "\n",
    "n_channels = len(dg_train.data.level.level)\n",
    "print('n_channels', n_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check I/O speed on single (empty) epoch\n",
    "for batch in train_loader:\n",
    "    inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "    print(inputs.shape, targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.train_nn_pytorch import CircUNet\n",
    "\n",
    "\"\"\"\n",
    "from src.train_nn_pytorch import SimpleCNN\n",
    "\n",
    "filters = [64, 64, 64, 64, 2] # last '2' for Z500, T850\n",
    "kernels = [5, 5, 5, 5, 5]\n",
    "activation = torch.nn.functional.elu\n",
    "mode='circular'\n",
    "\n",
    "model = SimpleCNN(filters=filters,\n",
    "                  kernels=kernels,\n",
    "                  channels=n_channels, \n",
    "                  activation=activation, \n",
    "                  mode=mode)\n",
    "\"\"\"\n",
    "\n",
    "filters =  [ [32], [32], [32], [32]] \n",
    "kernels =  [ [5],  [5], [5], [5] ]\n",
    "pooling = 2\n",
    "\n",
    "activation = torch.nn.functional.elu\n",
    "mode='circular'\n",
    "    \n",
    "model = CircUNet(in_channels=n_channels,\n",
    "                 filters=filters,\n",
    "                 kernels=kernels,\n",
    "                 pooling=pooling,\n",
    "                 activation=activation, \n",
    "                 out_channels=2,\n",
    "                 mode=mode)\n",
    "\n",
    "print('total #parameters: ', np.sum([np.prod(item.shape) for item in model.state_dict().values()]))\n",
    "print('output shape: ', model.forward(torch.zeros((7,2,32,64))).shape)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "train_again = True\n",
    "if train_again:\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    n_epochs, max_patience = 200, 20\n",
    "    best_loss, patience = np.inf, max_patience\n",
    "    best_state_dict = {}\n",
    "\n",
    "    epoch = 0\n",
    "    while True:\n",
    "\n",
    "        epoch += 1\n",
    "        if epoch > n_epochs:\n",
    "            break\n",
    "\n",
    "        print(f'epoch #{epoch}')\n",
    "        # Train for a single epoch.\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "            loss = F.mse_loss(model.forward(inputs), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Track convergence on validation set.\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            nb = 0\n",
    "            for batch in validation_loader:\n",
    "                inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "                val_loss += F.mse_loss(model.forward(inputs), targets)\n",
    "                nb += 1\n",
    "        val_loss /= nb\n",
    "        print(f'epoch #{epoch} || loss (last batch) {loss} || validation loss {val_loss}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            patience = max_patience\n",
    "            best_loss = val_loss\n",
    "            best_state_dict = deepcopy(model.state_dict()) # during early training will save every epoch\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience == 0:\n",
    "            model.load_state_dict(best_state_dict)\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), res_dir + model_fn)\n",
    "\n",
    "# if skip training, load model from disk\n",
    "else:\n",
    "    model.load_state_dict(torch.load(res_dir + model_fn, map_location=torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_nn_pytorch import create_predictions\n",
    "\n",
    "dg_test =  Dataset(x.sel(time=slice('2017', '2018')),\n",
    "                   var_dict,\n",
    "                   lead_time,\n",
    "                   mean=dg_train.mean, # make sure that model was trained \n",
    "                   std=dg_train.std,   # with same data as in dg_train, \n",
    "                   normalize=True)     # or else normalization is off!\n",
    "preds = create_predictions(model, dg_test, var_dict={'z' : None, 't' : None})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.score import compute_weighted_rmse, load_test_data\n",
    "\n",
    "z500_test = load_test_data(f'{datadir}geopotential_500/', 'z')\n",
    "t850_test = load_test_data(f'{datadir}temperature_850/', 't')\n",
    "rmse_z = compute_weighted_rmse(preds.z, z500_test.isel(time=slice(lead_time, None))).load()\n",
    "rmse_t = compute_weighted_rmse(preds.t, t850_test.isel(time=slice(lead_time, None))).load()\n",
    "print('RMSE z', rmse_z.values); print('RMSE t', rmse_t.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dg = dg_test\n",
    "\n",
    "# variable names for display in figure\n",
    "var_names = {'z' : 'geopotential at 500hPa', \n",
    "             't' : 'temperature at 850hPa'}\n",
    "\n",
    "# pick time stamps to visualize\n",
    "idx = [1000] # index relative to start time of dataset !\n",
    "\n",
    "for i in idx:\n",
    "    # predict for single time stamp\n",
    "    pred = model.forward(torch.tensor(dg[[i]][0],requires_grad=False).to(device)).detach().numpy()\n",
    "\n",
    "    plt.figure(figsize=(16,6))\n",
    "    for j in range(2):\n",
    "        plt.subplot(1,2,j+1)\n",
    "\n",
    "        # top: current state, middle: model-predicted future state, bottom: future state\n",
    "        j_ = dg._target_idx[j] # index for dg object in case first two dimensions not Z500, T850\n",
    "        plt.imshow(np.vstack((dg[[i]][1][0,j_,:,:], pred[0,j,:,:], dg[[i]][0][0,j_,:,:])))\n",
    "\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (1*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (2*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.yticks([pred.shape[2]//2, 3*pred.shape[2]//2, 5*pred.shape[2]//2],\n",
    "                   [f'+{lead_time}h true', f'+{lead_time}h est.', 'state'])\n",
    "        plt.axis([-0.5, pred.shape[3]-0.5, -0.5, 3*pred.shape[2]-0.5])\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(var_names[list(dg.var_dict.keys())[j]])\n",
    "        plt.title(dg.data.time.isel(time=i).values)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
