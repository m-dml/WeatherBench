{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# dev\n",
    "- reimplementing the 'Quickstart' notebook from S. Rasp for training in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from src.train_nn_pytorch import Dataset\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('using CUDA !')\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "\n",
    "lead_time = 3*24\n",
    "var_dict = {'z': None, 't': None}\n",
    "batch_size = 32\n",
    "\n",
    "datadir = '/gpfs/work/nonnenma/data/forecast_predictability/weatherbench/5_625deg/'\n",
    "res_dir = '/gpfs/work/nonnenma/results/forecast_predictability/weatherbench/5_625deg/'\n",
    "\n",
    "z500 = xr.open_mfdataset(f'{datadir}geopotential_500/*.nc', combine='by_coords')\n",
    "t850 = xr.open_mfdataset(f'{datadir}temperature_850/*.nc', combine='by_coords')\n",
    "dataset_list = [z500, t850]\n",
    "x = xr.merge(dataset_list, compat='override')\n",
    "n_channels = len(dataset_list) # = 1 if only loading one of geopotential Z500 and temperature T850\n",
    "\n",
    "# tbd: separating train and test datasets / loaders should be avoidable with the start/end arguments of Dataset!\n",
    "\n",
    "dg_train = Dataset(x.sel(time=slice('2015', '2015')), var_dict, lead_time, normalize=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg_train,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True)\n",
    "\n",
    "dg_validation =  Dataset(x.sel(time=slice('2016', '2016')), var_dict, lead_time,\n",
    "                        mean=dg_train.mean, std=dg_train.std, normalize=True)\n",
    "test_validation = torch.utils.data.DataLoader(\n",
    "    dg_validation,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class PeriodicConv2D(torch.nn.Conv2d):\n",
    "    \"\"\" Implementing 2D convolutional layer with mixed zero- and circular padding.\n",
    "    Uses circular padding along last axis (W) and zero-padding on second-last axis (H)\n",
    "    \n",
    "    \"\"\"\n",
    "    def conv2d_forward(self, input, weight):\n",
    "        if self.padding_mode == 'circular':\n",
    "            expanded_padding_circ = ( (self.padding[0] + 1) // 2, self.padding[0] // 2, 0, 0)\n",
    "            expanded_padding_zero = ( 0, 0, (self.padding[1] + 1) //2, self.padding[1] // 2 )\n",
    "            return F.conv2d(F.pad(F.pad(input, expanded_padding_circ, mode='circular'), \n",
    "                                  expanded_padding_zero, mode='constant'),\n",
    "                            weight, self.bias, self.stride,\n",
    "                            (0,0), self.dilation, self.groups)\n",
    "        return F.conv2d(input, weight, self.bias, self.stride,\n",
    "                        self.padding, self.dilation, self.groups)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, filters, kernels, channels, activation):\n",
    "        super(Net, self).__init__()\n",
    "        self.layers, in_ = [], channels\n",
    "        self.activation = activation\n",
    "        assert not np.any(kernels == 2), 'kernel size 2 not allowed for circular padding'\n",
    "        in_channels = [channels] + list(filters[:-1])\n",
    "        self.layers = torch.nn.ModuleList([PeriodicConv2D(i, f, k, padding=(k-1, k-1),\n",
    "                        padding_mode='circular') for i,f,k in zip(in_channels, filters, kernels)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.activation(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "net = Net(filters=[64, 64, 64, 64, n_channels], kernels=[5, 5, 5, 5, 5], \n",
    "          channels=n_channels, activation=torch.nn.functional.elu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 200\n",
    "epoch = 0\n",
    "while True:\n",
    "\n",
    "    epoch += 1\n",
    "    print(f'epoch #{epoch}')\n",
    "    # Train for a single epoch.\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "        loss = F.mse_loss(net.forward(inputs), targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "\n",
    "    # tbd: write early stopping from convergence on vakidation data\n",
    "    if epoch > n_epochs:\n",
    "        break\n",
    "        \n",
    "torch.save(net.state_dict(), res_dir + 'test_fccnn_3d_pytorch.pt')\n",
    "\n",
    "#net_rec = Net(filters=[64, 64, 64, 64, n_channels], kernels=[5, 5, 5, 5, 5], \n",
    "#          channels=n_channels, activation=torch.nn.functional.elu)\n",
    "#net_rec.load_state_dict(torch.load(res_dir + 'test_fccnn_3d_pytorch.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(res_dir + 'test_fccnn_3d_pytorch.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create a prediction and compute score\n",
    "\n",
    "Now that we have a model (albeit a crappy one) we can create a prediction. For this we need to create a forecast for each forecast initialization time in the testing range (2017-2018) and unnormalize it. We then convert the forecasts to a Xarray dataset which allows us to easily compute the RMSE. All of this is taken care of in the `create_predictions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create non-iterative predictions\"\"\"\n",
    "    preds = net.forward(torch.tensor(dg[np.arange(dg.__len__())][0]))\n",
    "    # Unnormalize\n",
    "    if dg.normalize:\n",
    "        preds = preds.detach().numpy() * dg.std.values[None,:,None,None] + dg.mean.values[None,:,None,None]\n",
    "    das = []\n",
    "    lev_idx = 0\n",
    "    for var, levels in dg.var_dict.items():\n",
    "        if levels is None:\n",
    "            das.append(xr.DataArray(\n",
    "                preds[:, lev_idx, :, :],\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            das.append(xr.DataArray(\n",
    "                preds[:, lev_idx:lev_idx+nlevs, :, :],\n",
    "                dims=['time', 'level' 'lat', 'lon'],\n",
    "                coords={'time': dg.valid_time, 'level': dg.ds.level, 'lat': dg.ds.lat, 'lon': dg.ds.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(das, compat='override')\n",
    "\n",
    "dg_test =  Dataset(x.sel(time=slice('2017', '2018')).isel(time=slice(0, None, 12)), var_dict, lead_time,\n",
    "                        mean=dg_train.mean, std=dg_train.std, normalize=True)\n",
    "dg_test.valid_time = dg_test.data.isel(time=slice(lead_time, None)).time\n",
    "preds = create_predictions(net, dg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from src.score import compute_weighted_rmse, load_test_data\n",
    "\n",
    "z500_test = load_test_data(f'{datadir}geopotential_500/', 'z')\n",
    "rmse_z = compute_weighted_rmse(preds.z, z500_test.isel(time=slice(lead_time, None))).load()\n",
    "rmse_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "time = '2017-03-02T00'\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n",
    "z500_test.sel(time=time).plot(ax=ax1)\n",
    "preds.sel(time=time).z.plot(ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The End\n",
    "\n",
    "This is the end of the quickstart guide. Please refer to the Jupyter notebooks in the `notebooks` directory for more examples. If you have questions, feel free to ask them as a Github Issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
