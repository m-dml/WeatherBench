{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# dev\n",
    "- reimplementing the 'Quickstart' notebook from S. Rasp for training in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from src.train_nn_pytorch import Dataset\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('using CUDA !')\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "\n",
    "datadir = '/gpfs/work/nonnenma/data/forecast_predictability/weatherbench/5_625deg/'\n",
    "res_dir = '/gpfs/work/nonnenma/results/forecast_predictability/weatherbench/5_625deg/'\n",
    "\n",
    "lead_time = 5*24\n",
    "batch_size = 32\n",
    "\n",
    "\"\"\"\n",
    "# geopotential and tempearture each at 11 levels \n",
    "# regression target is Z500 (level index i=6) and T850 (level index 9, stacked vector index 9+11=20)\n",
    "geop = xr.open_mfdataset(f'{datadir}geopotential/*.nc', combine='by_coords')\n",
    "temp = xr.open_mfdataset(f'{datadir}temperature/*.nc', combine='by_coords')\n",
    "\n",
    "# specific humidity, and wind component (u,v) volumes, each at 11 levels \n",
    "sphq = xr.open_mfdataset(f'{datadir}specific_humidity/*.nc', combine='by_coords')\n",
    "cowu = xr.open_mfdataset(f'{datadir}u_component_of_wind/*.nc', combine='by_coords')\n",
    "cowv = xr.open_mfdataset(f'{datadir}v_component_of_wind/*.nc', combine='by_coords')\n",
    "\"\"\"\n",
    "\n",
    "# indicent solar radiation and cloud cover fields, each single-level\n",
    "tisr = xr.open_mfdataset(f'{datadir}toa_incident_solar_radiation/*.nc', combine='by_coords')\n",
    "clou = xr.open_mfdataset(f'{datadir}total_cloud_cover/*.nc', combine='by_coords')\n",
    "\n",
    "# constants: orography, land-sea mask, soil type, lat2d and lon2d (each single-level)\n",
    "cnst = xr.open_mfdataset(f'{datadir}constants/*.nc', combine='by_coords')\n",
    "template = tisr.tisr\n",
    "T = len(template.time.values)\n",
    "dataarrays = {}\n",
    "for var in [cnst.orography, cnst.lsm, cnst.slt, cnst.lat2d, cnst.lon2d]:\n",
    "    values = np.tile(var.values.reshape(1,*var.values.shape), (T, 1, 1)).astype(np.float32)\n",
    "    dataarrays[var.name] = xr.DataArray(values, coords=template.coords, dims=template.dims, \n",
    "                                name=var.name,indexes=template.indexes)\n",
    "cnst = xr.Dataset(data_vars=dataarrays)\n",
    "\n",
    "# geopotential and tempearture each at target pressure levels \n",
    "z500 = xr.open_mfdataset(f'{datadir}geopotential_500/*.nc', combine='by_coords')\n",
    "t850 = xr.open_mfdataset(f'{datadir}temperature_850/*.nc', combine='by_coords')\n",
    "\n",
    "var_dict = {'z': None, 't': None, 'tisr' : None, 'tcc' : None, \n",
    "            'orography' : None, 'lsm' : None, 'slt' : None, 'lat2d' : None, 'lon2d': None}\n",
    "dataset_list = [z500, t850, tisr, clou, cnst]\n",
    "\n",
    "x = xr.merge(dataset_list, compat='override', fill_value=0) # fill_value for tisr !\n",
    "x = x.chunk({'time' : np.sum(x.chunks['time']), \n",
    "             'lat' : x.chunks['lat'], 'lon': x.chunks['lon']})\n",
    "\n",
    "# tbd: separating train and test datasets / loaders should be avoidable with the start/end arguments of Dataset!\n",
    "dg_train = Dataset(x.sel(time=slice('1979', '2015')), var_dict, lead_time, normalize=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg_train,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True)\n",
    "\n",
    "dg_validation =  Dataset(x.sel(time=slice('2016', '2016')), var_dict, lead_time,\n",
    "                        mean=dg_train.mean, std=dg_train.std, normalize=True)\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dg_validation,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False)\n",
    "\n",
    "n_channels = len(dg_train.data.level.level)\n",
    "print('n_channels', n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_nn_pytorch import SimpleCNN\n",
    "\n",
    "net = SimpleCNN(filters=[64, 64, 64, 64, 2], kernels=[5, 5, 5, 5, 5], \n",
    "          channels=n_channels, activation=torch.nn.functional.elu, mode='circular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "train_again = False\n",
    "\n",
    "if train_again:\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    n_epochs, max_patience = 200, 20\n",
    "    best_loss, patience = np.inf, max_patience\n",
    "    best_state_dict = {}\n",
    "\n",
    "    epoch = 0\n",
    "    while True:\n",
    "\n",
    "        epoch += 1\n",
    "        if epoch > n_epochs:\n",
    "            break\n",
    "\n",
    "        print(f'epoch #{epoch}')\n",
    "        # Train for a single epoch.\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "            loss = F.mse_loss(net.forward(inputs), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Track convergence on validation set.\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            nb = 0\n",
    "            for batch in validation_loader:\n",
    "                inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "                val_loss += F.mse_loss(net.forward(inputs), targets)\n",
    "                nb += 1\n",
    "        val_loss /= nb\n",
    "        print(f'epoch #{epoch} || loss (last batch) {loss} || validation loss {val_loss}')\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            patience = max_patience\n",
    "            best_loss = val_loss\n",
    "            best_state_dict = deepcopy(net.state_dict())\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "        if patience == 0:\n",
    "            net.load_state_dict(best_state_dict)\n",
    "            break\n",
    "\n",
    "    torch.save(net.state_dict(), res_dir + '9D_fccnn_5d_pytorch.pt')\n",
    "\n",
    "else:\n",
    "    net.load_state_dict(torch.load(res_dir + '9D_fccnn_5d_pytorch.pt', map_location=torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train_nn_pytorch import create_predictions\n",
    "\n",
    "dg_test =  Dataset(x.sel(time=slice('2017', '2018')), var_dict, lead_time,\n",
    "                        mean=dg_train.mean, std=dg_train.std, normalize=True)\n",
    "preds = create_predictions(net, dg_test, var_dict={'z' : None, 't' : None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.score import compute_weighted_rmse, load_test_data\n",
    "\n",
    "z500_test = load_test_data(f'{datadir}geopotential_500/', 'z')\n",
    "t850_test = load_test_data(f'{datadir}temperature_850/', 't')\n",
    "rmse_z = compute_weighted_rmse(preds.z, z500_test.isel(time=slice(lead_time, None))).load()\n",
    "rmse_t = compute_weighted_rmse(preds.t, t850_test.isel(time=slice(lead_time, None))).load()\n",
    "print('RMSE z', rmse_z.values); print('RMSE t', rmse_t.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "var_names = {'z' : 'geopotential at 500hPa',\n",
    "             't' : 'temperature at 850hPa'}\n",
    "idx = [0]\n",
    "for i in idx:\n",
    "    pred = net.forward(torch.tensor(dg_test[[i]][0],requires_grad=False).to(device)).detach().numpy()\n",
    "    #pred = pred*dg_train.std.values[None,:,None,None] + dg_train.mean.values[None,:,None,None]\n",
    "    plt.figure(figsize=(16,6))\n",
    "    for j in range(2):\n",
    "        plt.subplot(1,2,j+1)\n",
    "        plt.imshow(np.vstack((dg_test[[i]][1][0,j,:,:], pred[0,j,:,:], dg_test[[i]][0][0,j,:,:])))\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (1*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (2*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.yticks([pred.shape[2]//2, 3*pred.shape[2]//2, 5*pred.shape[2]//2], \n",
    "                   [f'+{lead_time}h true', f'+{lead_time}h est.', 'state'])\n",
    "        plt.axis([-0.5, pred.shape[3]-0.5, -0.5, 3*pred.shape[2]-0.5])\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(var_names[list(dg_test.var_dict.keys())[j]])\n",
    "        plt.title(dg_test.data.time.isel(time=i).values)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
