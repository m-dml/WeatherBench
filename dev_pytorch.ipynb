{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# dev\n",
    "- reimplementing the 'Quickstart' notebook from S. Rasp for training in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from src.pytorch.util import init_torch_device\n",
    "\n",
    "device = init_torch_device()\n",
    "\n",
    "datadir = '/gpfs/work/nonnenma/data/forecast_predictability/weatherbench/5_625deg/'\n",
    "res_dir = '/gpfs/work/nonnenma/results/forecast_predictability/weatherbench/5_625deg/'\n",
    "\n",
    "model_name = 'simpleResnet' # 'simpleResnet', 'tvfcnResnet50', 'cnnbn', 'Unetbn'\n",
    "\n",
    "lead_time = 3*24\n",
    "batch_size = 32\n",
    "\n",
    "train_years = ('1979', '2015')\n",
    "validation_years = ('2016', '2016')\n",
    "test_years = ('2017', '2018')\n",
    "\n",
    "var_dict = {'geopotential': ('z', [100, 200, 500, 850, 1000]),\n",
    "           'temperature': ('t', [100, 200, 500, 850, 1000]),\n",
    "           'u_component_of_wind': ('u', [100, 200, 500, 850, 1000]), \n",
    "           'v_component_of_wind': ('v', [100, 200, 500, 850, 1000]),\n",
    "           'constants': ['lsm','orography','lat2d']\n",
    "           }\n",
    "\n",
    "\"\"\"\n",
    "var_dict = {'geopotential': ('z', [500, 850]),\n",
    "           'temperature': ('t', [500, 850]),\n",
    "           'u_component_of_wind': ('u', [500, 850]), \n",
    "           'v_component_of_wind': ('v', [500, 850]),\n",
    "           'constants': ['lsm','orography','lat2d']\n",
    "           }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "target_var_dict = {'geopotential': 500, 'temperature': 850}\n",
    "\n",
    "filters = [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128]\n",
    "kernel_sizes = [7, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
    "\n",
    "past_times = [-6, -12]\n",
    "verbose = True\n",
    "loss_fun = 'mse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "from src.pytorch.Dataset import Dataset_dask, Dataset_xr\n",
    "\n",
    "x = xr.merge(\n",
    "[xr.open_mfdataset(f'{datadir}/{var}/*.nc', combine='by_coords')\n",
    " for var in var_dict.keys()],\n",
    "fill_value=0  # For the 'tisr' NaNs\n",
    ")\n",
    "dg_train = Dataset_dask(x.sel(time=slice(train_years[0], train_years[1])), var_dict, lead_time, \n",
    "                   normalize=False, res_dir=res_dir, train_years=train_years,\n",
    "                   target_var_dict=target_var_dict, past_times=past_times, verbose=verbose)\n",
    "x = xr.merge(\n",
    "[xr.open_mfdataset(f'{datadir}/{var}/*.nc', combine='by_coords')\n",
    " for var in var_dict.keys()],\n",
    "fill_value=0  # For the 'tisr' NaNs\n",
    ")\n",
    "dg_validation = Dataset_xr(x.sel(time=slice(validation_years[0], validation_years[1])), var_dict, lead_time,\n",
    "                        normalize=True, res_dir=res_dir, train_years=train_years, randomize_order=False,\n",
    "                        target_var_dict=target_var_dict, past_times=past_times, verbose=verbose)\n",
    "dg_test =  Dataset_xr(x.sel(time=slice(test_years[0], test_years[1])), var_dict, lead_time,\n",
    "                   normalize=True, mean=dg_validation.mean, std=dg_validation.std, randomize_order=False,\n",
    "                   target_var_dict=target_var_dict, past_times=past_times, verbose=verbose)\n",
    "print('chunks', dg_train.data.chunks)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X_stack = dask.array.stack([X for X,_ in batch], axis=0).compute()\n",
    "    Y_stack = dask.array.stack([y for _,y in batch], axis=0).compute()\n",
    "    X_stack = torch.tensor(X_stack, requires_grad=False)\n",
    "    Y_stack = torch.tensor(Y_stack, requires_grad=False)\n",
    "    return (X_stack, Y_stack)\n",
    "\n",
    "num_workers = int(train_years[1]) - int(train_years[0]) + 1\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg_train,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# check I/O speed on single (empty) epoch\n",
    "num_steps = 1\n",
    "t = time.time()\n",
    "for batch in train_loader:\n",
    "    if np.mod(num_steps, 1) == 0:\n",
    "        print('- #, time: ', num_steps, time.time() - t)\n",
    "    inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "    print(inputs.shape, targets.shape)\n",
    "    num_steps +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "[item.compute() for item in dg_train[np.arange(32)+dg_train.max_input_lag]]\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import xarray as xr\n",
    "import math\n",
    "\n",
    "class Dataset(torch.utils.data.IterableDataset):\n",
    "    r\"\"\"A class representing a :class:`Dataset`.\n",
    "\n",
    "    Base on DataGenerator() object written by S. Rasp (for tensorflow v1.x): \n",
    "    https://github.com/pangeo-data/WeatherBench/blob/ced939e20da0432bc816d64c34344e72f9b4cd17/src/train_nn.py#L18\n",
    "\n",
    "    .. note::\n",
    "      :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
    "      sampler that yields integral indices.  To make it work with a map-style\n",
    "      dataset with non-integral indices/keys, a custom sampler must be provided.\n",
    "    \"\"\"\n",
    "    def __init__(self, ds, var_dict, lead_time, mean=None, std=None, load=False,\n",
    "                 start=None, end=None, normalize=False, norm_subsample=1, randomize_order=True,\n",
    "                 target_var_dict={'geopotential' : 500, 'temperature' : 850}, \n",
    "                 dtype=np.float32, res_dir=None, train_years=None, past_times=[], verbose=False):\n",
    "\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.lead_time = lead_time\n",
    "        self.past_times = past_times\n",
    "        self.normalize = normalize\n",
    "        self.randomize_order = randomize_order\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # indexing for __getitem__ and __iter__ to find targets Z500, T850\n",
    "        assert np.all(var in var_dict.keys() for var in target_var_dict.keys())\n",
    "        assert np.all(level in var_dict[var][1] for var, level in target_var_dict.items())\n",
    "        \n",
    "        self.max_input_lag = -np.min(self.past_times) if len(self.past_times) > 0 else 0\n",
    "        if start is None or end is None:\n",
    "            start = np.max([0, self.max_input_lag])\n",
    "            end = self.ds.time.isel(time=slice(0, -self.lead_time)).values.shape[0]\n",
    "        assert end > start, \"this example code only works with end >= start\"\n",
    "        assert start >= self.max_input_lag\n",
    "        self.start, self.end = start, end\n",
    "\n",
    "        self.data = []\n",
    "        self.level_names = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for long_var, params in var_dict.items():\n",
    "            if long_var == 'constants':\n",
    "                for var in params:\n",
    "                    self.data.append(ds[var].expand_dims(\n",
    "                        {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "                    ).astype(dtype))\n",
    "                    self.level_names.append(var)\n",
    "            else:\n",
    "                var, levels = params\n",
    "                try:\n",
    "                    self.data.append(ds[var].sel(level=levels))\n",
    "                    self.level_names += [f'{var}_{level}' for level in levels]\n",
    "                except ValueError:\n",
    "                    self.data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "                    self.level_names.append(var)        \n",
    "\n",
    "        self.data = xr.concat(self.data, 'level')  # .transpose('time', 'lat', 'lon', 'level')\n",
    "        self.data['level_names'] = xr.DataArray(\n",
    "            self.level_names, dims=['level'], coords={'level': self.data.level})        \n",
    "        self.output_idxs = range(len(self.data.level))\n",
    "\n",
    "        if self.normalize:\n",
    "            if mean is None or std is None:\n",
    "                try:\n",
    "                    print('Loading means and standard deviations from disk')\n",
    "                    mean, std, level, level_names = load_mean_std(res_dir, var_dict, train_years)\n",
    "                    assert np.all( level_names == self.level_names )\n",
    "                    self.mean = xr.DataArray(mean, coords={'level': level}, dims=['level'])\n",
    "                    self.std = xr.DataArray(std, coords={'level': level}, dims=['level'])\n",
    "                except:\n",
    "                    print('WARNING! Could not load means and stds. Computing. Can take a while !')\n",
    "                    self.mean = self.data.isel(time=slice(0, None, norm_subsample)).mean(\n",
    "                        ('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "                    self.std = self.data.isel(time=slice(0, None, norm_subsample)).std(\n",
    "                        ('time', 'lat', 'lon')).compute() if std is None else std\n",
    "            else:\n",
    "                self.mean, self.std = mean, std\n",
    "            self.data = (self.data - self.mean) / self.std\n",
    "\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time+self.max_input_lag, None)).time\n",
    "\n",
    "        self._target_idx = []\n",
    "        for var, level in target_var_dict.items():\n",
    "            target_name = var_dict[var][0] + '_' + str(level)\n",
    "            self._target_idx += [np.where(np.array(self.level_names) == target_name)[0][0]]\n",
    "\n",
    "        # According to S. Rasp, this has to go after computation of self.mean, self.std:\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Generate one batch of data \"\"\"\n",
    "        assert np.min(index) >= self.start\n",
    "        idx = np.asarray(index)\n",
    "\n",
    "        X = self.data.data[idx,:,:,:]\n",
    "        y = self.data.data[idx + self.lead_time,:,:,:][:, self._target_idx, :, :]\n",
    "\n",
    "        if self.max_input_lag > 0:\n",
    "            Xl = [X]\n",
    "            for l in self.past_times:\n",
    "                Xl.append(self.data.data[idx+l,:,:,:])\n",
    "            X = dask.array.concatenate(Xl, axis=1) if len (idx) > 1 else dask.array.concatenate(Xl, axis=0)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\" Return iterable over data in random order \"\"\"\n",
    "        if torch.utils.data.get_worker_info() is None:\n",
    "            iter_start = torch.tensor(self.start, requires_grad=False, dtype=torch.int)\n",
    "            iter_end = torch.tensor(self.end, requires_grad=False, dtype=torch.int)  \n",
    "        else: \n",
    "            worker_info = torch.utils.data.get_worker_info()\n",
    "            worker_id, num_workers = worker_info.id, worker_info.num_workers\n",
    "            worker_yrs = math.ceil(len(self.data.chunks[0])/num_workers)\n",
    "            cumidx = np.concatenate(([0], np.cumsum(self.data.chunks[0])))\n",
    "            iter_start = cumidx[worker_id*worker_yrs] + self.start \n",
    "            iter_start = torch.tensor(iter_start, requires_grad=False, dtype=torch.int)\n",
    "            iter_end = min(cumidx[min((worker_id+1)*worker_yrs, len(self.data.chunks[0]))], self.end) \n",
    "            iter_end = torch.tensor(iter_end - self.lead_time, requires_grad=False, dtype=torch.int)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'worker stats: worker #{worker_id} / {num_workers}')\n",
    "                print('len(data.chunks)', len(self.data.chunks[0]))\n",
    "                print('#assigned years:', worker_yrs)\n",
    "                print('index start', iter_start)\n",
    "                print('index end', iter_end)\n",
    "\n",
    "        idx = np.arange(iter_start, iter_end)\n",
    "        if self.randomize_order:\n",
    "            idx = (torch.randperm(iter_end - iter_start) + iter_start).cpu().numpy()\n",
    "\n",
    "        X = self.data.data[idx,:,:,:]\n",
    "        y = self.data.data[idx + self.lead_time, :, :, :][:, self._target_idx, :, :]\n",
    "\n",
    "        if self.max_input_lag > 0:\n",
    "            Xl = [X]\n",
    "            for l in self.past_times:\n",
    "                Xl.append(self.data.data[idx+l,:,:,:])\n",
    "            X = dask.array.concatenate(Xl, axis=1) if len (idx) > 1 else dask.array.concatenate(Xl, axis=0)\n",
    "\n",
    "        return zip(X,y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.isel(time=slice(0, -self.lead_time)).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pytorch.util import load_data\n",
    "from src.pytorch.Dataset import collate_fn\n",
    "\n",
    "dg_train, dg_validation, dg_test = load_data(\n",
    "    var_dict=var_dict, lead_time=lead_time,\n",
    "    train_years=(train_years[0], train_years[1]), \n",
    "    validation_years=(validation_years[0], validation_years[1]), \n",
    "    test_years=(test_years[0], test_years[1]),\n",
    "    target_var_dict=target_var_dict, datadir=datadir, res_dir=res_dir,\n",
    "    past_times=past_times, verbose=True\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dg_validation,\n",
    "    batch_size=batch_size,\n",
    "    drop_last=False)    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg_train,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=int(train_years[1]) - int(train_years[0]) + 1,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True)  \n",
    "\n",
    "n_channels = len(dg_train.data.level.level) * (len(dg_train.past_times)+1)\n",
    "print('n_channels', n_channels)\n",
    "\n",
    "#model_fn = f'{n_channels}D_fc{model_name}_{lead_time//24}d_pytorch.pt' # file name for saving/loading prediction model\n",
    "model_fn = f'{n_channels}D_fc{model_name}_{lead_time//24}d_pytorch_lrdecay_weightdecay_normed_test2.pt' # file name for saving/loading prediction model\n",
    "print('model filename', model_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# check I/O speed on single (empty) epoch\n",
    "num_steps = 0\n",
    "t = time.time()\n",
    "for batch in train_loader:\n",
    "    if np.mod(num_steps, 1) == 0:\n",
    "        print('- #, time: ', num_steps, time.time() - t)\n",
    "    inputs, targets = batch[0].to(device), batch[1].to(device)\n",
    "    print(inputs.shape, targets.shape)\n",
    "    num_steps +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.pytorch.util import named_network\n",
    "model, model_forward = named_network(model_name, n_channels, len(target_var_dict), \n",
    "                                     filters=filters, kernel_sizes=kernel_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = 'models/multi_delay_test/multi_delay_test_33D_fcsimpleResnet_72h.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.pytorch.train import train_model, loss_function\n",
    "\n",
    "train_again = False\n",
    "if train_again:\n",
    "    loss_fun = loss_function(loss_fun)\n",
    "    training_outputs = train_model(model, train_loader, validation_loader, device, model_forward,\n",
    "                    loss_fun=loss_fun, lr=5e-4, lr_min=1e-5, lr_decay=0.2, weight_decay=1e-5,\n",
    "                    max_epochs=200, max_patience=20, max_lr_patience=5, eval_every=2000,\n",
    "                    verbose=True, save_dir=res_dir + model_fn)\n",
    "\n",
    "# if skip training, load model from disk\n",
    "else:\n",
    "    model.load_state_dict(torch.load(res_dir + model_fn, map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.score import compute_weighted_rmse, load_test_data\n",
    "z500_test = load_test_data(f'{datadir}geopotential_500/', 'z')\n",
    "t850_test = load_test_data(f'{datadir}temperature_850/', 't')\n",
    "z500_test.isel(time=slice(lead_time+dg_test.max_input_lag, None)).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pytorch.train import calc_val_loss\n",
    "print('validation loss:', calc_val_loss(validation_loader, model_forward, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.pytorch.train_nn import create_predictions\n",
    "from src.score import compute_weighted_rmse, load_test_data\n",
    "\n",
    "preds = create_predictions(model,\n",
    "                           dg_test,\n",
    "                           var_dict={'z' : None, 't' : None},\n",
    "                           batch_size=100,\n",
    "                           model_forward=model_forward,\n",
    "                           verbose=True)\n",
    "\n",
    "z500_test = load_test_data(f'{datadir}geopotential_500/', 'z')\n",
    "t850_test = load_test_data(f'{datadir}temperature_850/', 't')\n",
    "rmse_z = compute_weighted_rmse(preds.z, z500_test.isel(time=slice(lead_time+dg_test.max_input_lag, None))).load()\n",
    "rmse_t = compute_weighted_rmse(preds.t, t850_test.isel(time=slice(lead_time+dg_test.max_input_lag, None))).load()\n",
    "print('RMSE z', rmse_z.values); print('RMSE t', rmse_t.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_z = compute_weighted_rmse(preds.z, z500_test.isel(time=slice(lead_time+dg_test.max_input_lag None))).load()\n",
    "rmse_t = compute_weighted_rmse(preds.t, t850_test.isel(time=slice(lead_time+max_input_lag, None))).load()\n",
    "print('RMSE z', rmse_z.values); print('RMSE t', rmse_t.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dg = dg_test\n",
    "\n",
    "# variable names for display in figure\n",
    "var_names = {'geopotential' : 'geopotential at 500hPa', \n",
    "             'temperature' : 'temperature at 850hPa'}\n",
    "\n",
    "# pick time stamps to visualize\n",
    "idx = [2000] # index relative to start time of dataset !\n",
    "\n",
    "for i in idx:\n",
    "    pre = dg[[i]][0]\n",
    "    post = dg[[i]][1]\n",
    "    # predict for single time stamp\n",
    "    pred = model_forward(torch.tensor(pre,requires_grad=False).to(device)).detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(16,6))\n",
    "    for j in range(2):\n",
    "        plt.subplot(1,2,j+1)\n",
    "\n",
    "        # top: current state, middle: model-predicted future state, bottom: future state\n",
    "        j_ = dg._target_idx[j] # index for dg object in case first two dimensions not Z500, T850\n",
    "        plt.imshow(np.vstack((post[0,j,:,:], pred[0,j,:,:], pre[0,j_,:,:])))\n",
    "\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (1*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (2*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.yticks([pred.shape[2]//2, 3*pred.shape[2]//2, 5*pred.shape[2]//2],\n",
    "                   [f'+{lead_time}h true', f'+{lead_time}h est.', 'state'])\n",
    "        plt.axis([-0.5, pred.shape[3]-0.5, -0.5, 3*pred.shape[2]-0.5])\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(var_names[list(dg.var_dict.keys())[j]])\n",
    "        plt.title(dg.data.time.isel(time=i).values)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dg = dg_test\n",
    "\n",
    "# variable names for display in figure\n",
    "var_names = {'geopotential' : 'geopotential at 500hPa', \n",
    "             'temperature' : 'temperature at 850hPa'}\n",
    "\n",
    "# pick time stamps to visualize\n",
    "idx = [2000] # index relative to start time of dataset !\n",
    "\n",
    "for i in idx:\n",
    "    pre = dg[[i]][0]\n",
    "    post = dg[[i]][1]\n",
    "    # predict for single time stamp\n",
    "    pred = model_forward(torch.tensor(pre,requires_grad=False).to(device)).detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(16,6))\n",
    "    for j in range(2):\n",
    "        plt.subplot(1,2,j+1)\n",
    "\n",
    "        # top: current state, middle: model-predicted future state, bottom: future state\n",
    "        j_ = dg._target_idx[j] # index for dg object in case first two dimensions not Z500, T850\n",
    "        plt.imshow(np.vstack((post[0,j,:,:], pred[0,j,:,:], pre[0,j_,:,:])))\n",
    "\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (1*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.plot([0.5, pred.shape[3]+.5], (2*pred.shape[2]-0.5)*np.ones(2), 'k', linewidth=1.5)\n",
    "        plt.yticks([pred.shape[2]//2, 3*pred.shape[2]//2, 5*pred.shape[2]//2],\n",
    "                   [f'+{lead_time}h true', f'+{lead_time}h est.', 'state'])\n",
    "        plt.axis([-0.5, pred.shape[3]-0.5, -0.5, 3*pred.shape[2]-0.5])\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(var_names[list(dg.var_dict.keys())[j]])\n",
    "        plt.title(dg.data.time.isel(time=i).values)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model survery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "exp_ids = ['resnet_baseline', 'resnet_baseline_no_L2',  'resnet_latmse', 'multi_delay_test']\n",
    "def find_weights(fn):\n",
    "    return fn[-4:] == 'h.pt'\n",
    "dims = [23, 23, 23, 33]\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "for exp_id, dim in zip(exp_ids, dims):\n",
    "    save_dir = res_dir + 'models/' + exp_id + '/'\n",
    "\n",
    "    model_fn = list(filter(find_weights, os.listdir(save_dir)))[0]\n",
    "    lead_time = model_fn[-6:-4]\n",
    "    training_outputs = np.load(save_dir + '_training_outputs' + '.npy', allow_pickle=True)[()]\n",
    "\n",
    "    try:\n",
    "        training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "        RMSEs = np.load(save_dir + model_fn[:-3] + '_RMSE_zt.npy')\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.semilogy(validation_loss, label=exp_id + f' ({dim}D)')\n",
    "        plt.title('training')\n",
    "        \n",
    "        plt.subplot(1,4,3)\n",
    "        plt.plot([0,1], RMSEs[0]*np.ones(2), label=exp_id)\n",
    "        plt.title(f'RMSE {lead_time}h, z 500')\n",
    "        plt.xticks([])\n",
    "        plt.axis([-0.1, 1.1, 0, 600])\n",
    "        \n",
    "        plt.subplot(1,4,4)\n",
    "        plt.plot([0,1], RMSEs[1]*np.ones(2), label=exp_id)\n",
    "        plt.title(f'RMSE {lead_time}h, t 850')\n",
    "        plt.xticks([])\n",
    "        plt.axis([-0.1, 1.1, 0, 3.0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.ylabel('validation error')\n",
    "plt.legend()\n",
    "#plt.subplot(1,4,3)\n",
    "#plt.legend()\n",
    "fig.patch.set_facecolor('xkcd:white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE per pixel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RMSEs_z = np.sqrt(np.mean((preds[:,0,:,:] - z500_test.isel(time=slice(lead_time, None)))**2, axis=0))\n",
    "RMSEs_t = np.sqrt(np.mean((preds[:,1,:,:] - t850_test.isel(time=slice(lead_time, None)))**2, axis=0))\n",
    "\n",
    "\n",
    "weights_lat = np.cos(np.deg2rad(z500_test.lat))\n",
    "weights_lat /= weights_lat.mean()\n",
    "\n",
    "wRMSEs_z = np.sqrt(\n",
    "    np.mean( ((preds[:,0,:,:] - z500_test.isel(time=slice(lead_time, None)))**2)*weights_lat, \n",
    "            axis=0))\n",
    "wRMSEs_t = np.sqrt(\n",
    "    np.mean( ((preds[:,1,:,:] - t850_test.isel(time=slice(lead_time, None)))**2)*weights_lat, \n",
    "            axis=0))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(RMSEs_z)\n",
    "plt.title('RMSEs Z500')\n",
    "plt.colorbar()\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(RMSEs_t)\n",
    "plt.title('RMSEs T850')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(wRMSEs_z)\n",
    "plt.title('weighted RMSEs Z500')\n",
    "plt.colorbar()\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(wRMSEs_t)\n",
    "plt.title('weighted RMSEs T850')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSEs per time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RMSEs_z = np.sqrt(np.mean((preds[:,0,:,:] - z500_test.isel(time=slice(lead_time, None)))**2, axis=[1,2]))\n",
    "RMSEs_t = np.sqrt(np.mean((preds[:,1,:,:] - t850_test.isel(time=slice(lead_time, None)))**2, axis=[1,2]))\n",
    "\n",
    "\n",
    "weights_lat = np.cos(np.deg2rad(z500_test.lat))\n",
    "weights_lat /= weights_lat.mean()\n",
    "\n",
    "wRMSEs_z = np.sqrt(\n",
    "    np.mean( ((preds[:,0,:,:] - z500_test.isel(time=slice(lead_time, None)))**2)*weights_lat, \n",
    "            axis=[1,2]))\n",
    "wRMSEs_t = np.sqrt(\n",
    "    np.mean( ((preds[:,1,:,:] - t850_test.isel(time=slice(lead_time, None)))**2)*weights_lat, \n",
    "            axis=[1,2]))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(RMSEs_z)\n",
    "plt.title('RMSEs Z500')\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(RMSEs_t)\n",
    "plt.title('RMSEs T850')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(wRMSEs_z)\n",
    "plt.title('weighted RMSEs Z500')\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(wRMSEs_t)\n",
    "plt.title('weighted RMSEs T850')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quickplot of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dg = dg_test\n",
    "\n",
    "# variable names for display in figure\n",
    "var_names = {'geopotential' : 'geopotential at 500hPa', \n",
    "             'temperature' : 'temperature at 850hPa'}\n",
    "\n",
    "# pick time stamps to visualize\n",
    "idx = [2000, 2024, 2048, 2072, 2096, 2120] # index relative to start time of dataset !\n",
    "pre = dg[idx][0]\n",
    "preds = model_forward(torch.tensor(pre,requires_grad=False).to(device)).detach().numpy() \n",
    "    \n",
    "plt.figure(figsize=(16,3))\n",
    "idx_plot = [0,1,2,3,4]\n",
    "plt.imshow(np.hstack(preds[idx_plot,0,:,:]))\n",
    "for i in range(1,len(idx_plot)):\n",
    "    plt.plot(i*64*np.ones(2), [0, 31], 'k')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,3))\n",
    "idx_plot = [0,1,2,3,4]\n",
    "plt.imshow(np.hstack(preds[idx_plot,1,:,:]))\n",
    "for i in range(1,len(idx_plot)):\n",
    "    plt.plot(i*64*np.ones(2), [0, 31], 'k')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "#plt.savefig('/gpfs/home/nonnenma/projects/seasonal_forecasting/results/weatherbench/figs/T850_example_dreds_N5_dt24h.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
